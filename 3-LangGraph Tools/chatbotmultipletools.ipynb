{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9afabd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4644a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=700)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv,description=\"Query arxiv papers\")\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ed9db1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-07-22\\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\nSummary: The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example,\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "408f6af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki,description=\"Query wikipedia\")\n",
    "print(wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90d13c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff589bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily search tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily.invoke(\"Tell me about ISRO 101st mission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c90f5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all these tools in the list\n",
    "tools = [arxiv,wiki,tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c905b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the LLM mode\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c975969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI QA Engineering, also known as Artificial Intelligence Quality Assurance Engineering, is a subfield of software engineering that focuses on using artificial intelligence (AI) and machine learning (ML) technologies to improve the quality and efficiency of software testing and quality assurance (QA) processes.\\n\\n**Key Responsibilities:**\\n\\nAI QA Engineers are responsible for designing, implementing, and maintaining AI-powered testing solutions that can automatically identify bugs, defects, and issues in software applications. Their key responsibilities include:\\n\\n1. **Testing Automation**: Developing and maintaining automated testing frameworks, scripts, and tools to reduce manual testing efforts.\\n2. **AI-Driven Testing**: Designing and implementing AI-powered testing solutions that can identify bugs, defects, and issues in software applications.\\n3. **Predictive Maintenance**: Using AI and ML algorithms to predict potential issues and defects in software applications before they occur.\\n4. **Continuous Integration and Continuous Deployment (CI/CD)**: Integrating AI-powered testing into CI/CD pipelines to ensure seamless testing and deployment of software applications.\\n5. **Collaboration**: Working closely with developers, testers, and other stakeholders to ensure AI-powered testing solutions are aligned with business goals and requirements.\\n\\n**Technologies and Tools:**\\n\\nAI QA Engineers use a range of technologies and tools, including:\\n\\n1. **Programming Languages**: Python, Java, C++, etc.\\n2. **AI and ML Frameworks**: TensorFlow, PyTorch, Keras, etc.\\n3. **Testing Frameworks**: Selenium, Appium, TestNG, etc.\\n4. **CI/CD Tools**: Jenkins, GitLab CI/CD, Travis CI, etc.\\n5. **Cloud Platforms**: AWS, Azure, Google Cloud, etc.\\n\\n**Skills and Qualifications:**\\n\\nTo become an AI QA Engineer, you should have:\\n\\n1. **Strong programming skills**: In languages such as Python, Java, or C++.\\n2. **AI and ML knowledge**: Understanding of AI and ML concepts, including machine learning algorithms and deep learning techniques.\\n3. **Testing expertise**: Experience with testing frameworks, scripts, and tools.\\n4. **CI/CD knowledge**: Understanding of CI/CD pipelines and automation tools.\\n5. **Communication skills**: Ability to collaborate with developers, testers, and other stakeholders.\\n\\n**Career Path:**\\n\\nThe career path for an AI QA Engineer can involve:\\n\\n1. **Junior AI QA Engineer**: Assisting senior engineers with AI-powered testing solutions.\\n2. **Senior AI QA Engineer**: Designing and implementing AI-powered testing solutions.\\n3. **AI QA Engineering Lead**: Leading teams of AI QA engineers and overseeing AI-powered testing initiatives.\\n4. **AI QA Engineering Manager**: Managing AI QA engineering teams and overseeing AI-powered testing strategies.\\n\\n**Job Outlook:**\\n\\nThe demand for AI QA Engineers is growing rapidly, driven by the increasing adoption of AI and ML technologies in software development. According to Glassdoor, the average salary for an AI QA Engineer in the United States is around $115,000 per year.\\n\\n**Conclusion:**\\n\\nAI QA Engineering is a rapidly evolving field that requires a unique blend of technical, business, and communication skills. As the demand for AI-powered testing solutions continues to grow, the career prospects for AI QA Engineers are highly promising.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 652, 'prompt_tokens': 41, 'total_tokens': 693, 'completion_time': 0.869333333, 'prompt_time': 0.002587132, 'queue_time': 0.106959738, 'total_time': 0.871920465}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None}, id='run--b15b181b-7d98-4d73-a39a-205dbd9d4ae1-0', usage_metadata={'input_tokens': 41, 'output_tokens': 652, 'total_tokens': 693})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me about AI QA Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7668aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb861a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_e3ah', 'function': {'arguments': '{\"query\": \"S-550 latest news\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 394, 'total_tokens': 416, 'completion_time': 0.029333333, 'prompt_time': 0.01662519, 'queue_time': 0.048234059999999995, 'total_time': 0.045958523}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--aff134dd-d78e-4691-a6f2-993093eab928-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'S-550 latest news'}, 'id': 'call_e3ah', 'type': 'tool_call'}], usage_metadata={'input_tokens': 394, 'output_tokens': 22, 'total_tokens': 416})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is the latest news about S-550\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5dd1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow\n",
    "\n",
    "# state schema\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage # human message or AI message\n",
    "from typing import Annotated # labelling\n",
    "from langgraph.graph.message import add_messages # reducers in langgraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2c7f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire chatbot with langgraph\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node definition\n",
    "\n",
    "def llm_calling_tool(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm_calling_tool\",llm_calling_tool)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# edges\n",
    "builder.add_edge(START,\"llm_calling_tool\")\n",
    "builder.add_conditional_edges(\n",
    "    \"llm_calling_tool\",\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\",END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# view\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eebe936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1706.03762\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_4nvk)\n",
      " Call ID: call_4nvk\n",
      "  Args:\n",
      "    query: 1706.03762\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parall\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\":\"1706.03762\"})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf5e327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about Attention is all you need\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_ebjw)\n",
      " Call ID: call_ebjw\n",
      "  Args:\n",
      "    query: Attention is all you need\n",
      "  wikipedia (call_2xvn)\n",
      " Call ID: call_2xvn\n",
      "  Args:\n",
      "    query: Attention is all you need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2024-07-22\n",
      "Title: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "Authors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
      "Summary: The inference demand for LLMs has skyrocketed in recent months, and serving\n",
      "models with low latencies remains challenging due to the quadratic input length\n",
      "complexity of the attention layers. In this work, we investigate the effect of\n",
      "dropping MLP and attention layers at inference time on the performance of\n",
      "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
      "decreases performance but leads to the best speedups alongside dropping entire\n",
      "layers. For example,\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wi\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\":\"What do you know about Attention is all you need\"})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d62878b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# node definition\n",
    "\n",
    "def llm_calling_tool(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm_calling_tool\",llm_calling_tool)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# edges\n",
    "builder.add_edge(START,\"llm_calling_tool\")\n",
    "builder.add_conditional_edges(\n",
    "    \"llm_calling_tool\",\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\",\"llm_calling_tool\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# view\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8aa3743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about Attention is all you need\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_y61f)\n",
      " Call ID: call_y61f\n",
      "  Args:\n",
      "    query: Attention is all you need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2024-07-22\n",
      "Title: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "Authors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
      "Summary: The inference demand for LLMs has skyrocketed in recent months, and serving\n",
      "models with low latencies remains challenging due to the quadratic input length\n",
      "complexity of the attention layers. In this work, we investigate the effect of\n",
      "dropping MLP and attention layers at inference time on the performance of\n",
      "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
      "decreases performance but leads to the best speedups alongside dropping entire\n",
      "layers. For example,\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia (call_ft1e)\n",
      " Call ID: call_ft1e\n",
      "  Args:\n",
      "    query: Attention is all you need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia\n",
      "\n",
      "Page: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wi\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_qz8y)\n",
      " Call ID: call_qz8y\n",
      "  Args:\n",
      "    query: Attention is all you need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Attention Is All You Need - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\", \"content\": \"\\\"Attention Is All You Need\\\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is considered a foundational[5] paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as [...] ^ a b c d e f Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, ≈Åukasz; Polosukhin, Illia (December 2017). \\\"Attention is All you Need\\\". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). 31st Conference on Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol.¬†30. Curran Associates, Inc. arXiv:1706.03762. [...] In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \\\"Attention is all you need\\\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads\", \"score\": 0.91156644}, {\"title\": \"Attention Is All You Need - arXiv\", \"url\": \"https://arxiv.org/html/1706.03762v7\", \"content\": \"Report issue for preceding element\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [[4](#bib.bib4), [27](#bib.bib27), [28](#bib.bib28), [22](#bib.bib22)]. [...] As noted in Table [1](#S4.T1 \\\"Table 1 ‚Ä£ 4 Why Self-Attention ‚Ä£ Attention Is All You Need\\\"), a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O‚Å¢(n)ùëÇùëõO(n)italic\\\\_O ( italic\\\\_n ) sequential operations. [...] ,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqW\", \"score\": 0.78512776}, {\"title\": \"[1706.03762] Attention Is All You Need - arXiv\", \"url\": \"https://arxiv.org/abs/1706.03762\", \"content\": \"|  |  |\\n| --- | --- |\\n| Comments: | 15 pages, 5 figures |\\n| Subjects: | Computation and Language (cs.CL); Machine Learning (cs.LG) |\\n| Cite as: | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) [cs.CL] |\\n|  | (or  [arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7) [cs.CL] for this version) |\\n|  | <https://doi.org/10.48550/arXiv.1706.03762> Focus to learn more  arXiv-issued DOI via DataCite |\\n\\n## Submission history [...] [![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1706.03762&description=Attention Is All You Need \\\"Bookmark on BibSonomy\\\")\\n[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1706.03762&title=Attention Is All You Need \\\"Bookmark on Reddit\\\")\\n\\n\\n\\nBibliographic Tools\\n\\n# Bibliographic and Citation Tools [...] From: Llion Jones [[view email](/show-email/f53b7360/1706.03762)]   \\n **[[v1]](/abs/1706.03762v1)**\\nMon, 12 Jun 2017 17:57:34 UTC (1,102 KB)  \\n**[[v2]](/abs/1706.03762v2)**\\nMon, 19 Jun 2017 16:49:45 UTC (1,125 KB)  \\n**[[v3]](/abs/1706.03762v3)**\\nTue, 20 Jun 2017 05:20:02 UTC (1,125 KB)  \\n**[[v4]](/abs/1706.03762v4)**\\nFri, 30 Jun 2017 17:29:30 UTC (1,124 KB)  \\n**[[v5]](/abs/1706.03762v5)**\\nWed, 6 Dec 2017 03:30:32 UTC (1,124 KB)  \\n**[[v6]](/abs/1706.03762v6)**\", \"score\": 0.78154784}, {\"title\": \"\\\"But Google is the author of 'Attention Is All You Need', they know ...\", \"url\": \"https://www.reddit.com/r/ChatGPT/comments/125i04d/but_google_is_the_author_of_attention_is_all_you/\", \"content\": \"ChatGPT and all other LLM's rely heavily on the neural network building block called Transformer. It consists of MultiHeadAttention layers, therefore the name of the paper \\\"Attention Is All You Need\\\". A transformer model allows to input the sequences in parallel. It is remarkable, as pervious sequence to sequence models (as input text, to response text is) required going through the input sequence word by word in sequence. So transformer models are so much cheaper and faster to train, which\", \"score\": 0.7587435}, {\"title\": \"[PDF] Attention is All you Need - NIPS papers\", \"url\": \"https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\", \"content\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. [...] 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\", \"score\": 0.75659186}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The paper \"Attention Is All You Need\" was published in 2017 and introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. The paper is considered a foundational paper in modern artificial intelligence and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as machine translation, reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "\n",
      "The transformer model allows for the input of sequences in parallel, which is a significant improvement over previous sequence-to-sequence models that required going through the input sequence word by word in sequence. This makes the transformer model cheaper and faster to train.\n",
      "\n",
      "The attention mechanism is a key component of the transformer model, and it is used to compute a representation of the sequence by mapping a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "Overall, the paper \"Attention Is All You Need\" has had a significant impact on the field of artificial intelligence and has led to the development of many new AI models and applications.\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\":\"What do you know about Attention is all you need\"})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a319bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the current weather condition in Patna,India\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_ka1q)\n",
      " Call ID: call_ka1q\n",
      "  Args:\n",
      "    query: Patna India weather today\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Weather in Patna, India\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'Patna', 'region': 'Bihar', 'country': 'India', 'lat': 25.6, 'lon': 85.1167, 'tz_id': 'Asia/Kolkata', 'localtime_epoch': 1747564364, 'localtime': '2025-05-18 16:02'}, 'current': {'last_updated_epoch': 1747564200, 'last_updated': '2025-05-18 16:00', 'temp_c': 33.1, 'temp_f': 91.6, 'is_day': 1, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/day/143.png', 'code': 1030}, 'wind_mph': 4.3, 'wind_kph': 6.8, 'wind_degree': 33, 'wind_dir': 'NNE', 'pressure_mb': 1002.0, 'pressure_in': 29.59, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 56, 'cloud': 50, 'feelslike_c': 31.3, 'feelslike_f': 88.3, 'windchill_c': 40.7, 'windchill_f': 105.3, 'heatindex_c': 41.8, 'heatindex_f': 107.2, 'dewpoint_c': 14.8, 'dewpoint_f': 58.6, 'vis_km': 5.0, 'vis_miles': 3.0, 'uv': 1.0, 'gust_mph': 4.9, 'gust_kph': 7.9}}\", \"score\": 0.9759114}, {\"title\": \"Patna May 2025 Historical Weather Data (Bihar, India)\", \"url\": \"https://weatherspark.com/h/m/111069/2025/5/Historical-Weather-in-May-2025-in-Patna-Bihar-India\", \"content\": \"The data for this report comes from the Lok Nayak Jayaprakash Airport. See all nearby weather stations\\n\\nLatest Report √¢¬Ä¬î 3:30 PM\\n\\nSun, May 18, 2025¬†¬†¬†¬†33 min ago¬†¬†¬†¬†UTC 10:00\\n\\nCall Sign VEPT\\n\\nTemp.\\n\\n91√Ç¬∞F\\n\\nDew Pt. | 73√Ç¬∞Foppressive\\nRel. Humidity | 56%\\nPrecipitation\\n\\nNo Report\\n\\nWind\\n\\n10.4 mph\\n\\nWind Dir. | 80 deg, E\\nCloud Cover\\n\\nPartly Cloudy\\n\\n\\n10,000 ft\\n\\n | Mostly Clear 2,000 ft\\nVis. | 3.11 mi\\nAlt. | 29.59 inHg\\nRaw: VEPT 181000Z 08009KT 5000 HZ FEW020 SCT100 33/23 Q1002 NOSIG [...] Sun, May 18 | Haze, Mist |  | HZ, BR\\nMay 2025 |  | Illumination | Moonrise | Moonset | Moonrise | Meridian Passing | Distance\\n1 |  | 18% | 7:58 AM | ENE | 10:36 PM | WNW | - | 3:17 PM | N | 229,421 mi\\n2 |  | 28% | 9:03 AM | ENE | 11:31 PM | WNW | - | 4:19 PM | N | 233,272 mi\\n3 |  | 39% | 10:08 AM | ENE | - | - | 5:16 PM | S | 237,243 mi\\n4 |  | 50% | - | 12:18 AM | WNW | 11:10 AM | ENE | 6:07 PM | S | 241,027 mi\\n5 |  | 60% | - | 12:58 AM | WNW | 12:09 PM | ENE | 6:53 PM | S | 244,403 mi [...] TodayYesterdayMay20251940195019601970198019902000201020202016201720182019202020212022202320242025SpringSummerFallWinterJanFebMarAprMayJunJulAugSepOctNovDec12345678910111213141516171819202122232425262728293031May 2025 Weather History in PatnaBihar, IndiaThe data for this report comes from the Lok Nayak Jayaprakash Airport.See all nearby weather stationsLatest Report√¢¬Ä¬î3:30 PMSun, May 18, 2025¬†¬†¬†¬†33 min ago¬†¬†¬†¬†UTC 10:00Call Sign VEPTTemp.91√Ç¬∞FhotDew Pt.73√Ç¬∞FoppressiveRel.\", \"score\": 0.95176035}, {\"title\": \"Patna Weather Forecast 18 May 2025 - Times of India\", \"url\": \"https://timesofindia.indiatimes.com/weather/patna-weather-forecast-today/800001\", \"content\": \"Today's Weather in Patna: In Patna today, the weather is expected to be Haze with a maximum temperature of 35¬∞C and a minimum of 26¬∞C. Sunrise in Patna is\", \"score\": 0.95100147}, {\"title\": \"Patna weather in May 2025 ‚ãÜ Patna temperature ... - Meteoprog.com\", \"url\": \"https://www.meteoprog.com/weather/Patna/month/may/\", \"content\": \"Patna (India) weather in May 2025 ‚òÄÔ∏è Accurate weather forecast for Patna in ... 18 May. +39¬∞+28¬∞. 19 May. +38¬∞+28¬∞. 20 May. +38¬∞+28¬∞. 21 May. +40¬∞+28¬∞. 22 May. +40\", \"score\": 0.9477354}, {\"title\": \"Patna Weather Today (Sunday, May 18, 2025)\", \"url\": \"https://www.indiatoday.in/weather/patna-weather-forecast-today\", \"content\": \"The minimum temperature in Patna today is likely to hover around 27 degrees Celsius, while the maximum temperature might reach 42 degrees Celsius. The mercury level is expected to hover around 39 degrees Celsius throughout the day, with the wind speed around 9.29. The wind will move around 94 degrees with a gust speed of 14.43. The sunrise time is 05:02 AM, while it will set at 06:29 PM on Sunday. As per the seven-day weather prediction, the temperature in Patna is likely to reach 42 degrees [...] Celsius on Sunday, 39 degrees Celsius on Monday, 41 degrees Celsius on Tuesday, 41 degrees Celsius on Wednesday, 40 degrees Celsius on Thursday, 41 degrees Celsius on Friday and 42 degrees Celsius on Saturday.\", \"score\": 0.9387167}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current weather condition in Patna, India is Mist with a temperature of 33.1¬∞C. The humidity is 56%, and the wind speed is 6.8 km/h.\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\":\"What is the current weather condition in Patna,India\"})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
